{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpQSeFqrEFjt"
      },
      "source": [
        "## Cài đặt môi trường"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2025-07-16T09:40:56.733026Z",
          "iopub.status.busy": "2025-07-16T09:40:56.732719Z",
          "iopub.status.idle": "2025-07-16T09:41:34.414742Z",
          "shell.execute_reply": "2025-07-16T09:41:34.413906Z",
          "shell.execute_reply.started": "2025-07-16T09:40:56.733002Z"
        },
        "id": "8HG_tAXSdibb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q flwr[simulation]\n",
        "!pip install -q flwr-datasets[vision]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:41:34.417087Z",
          "iopub.status.busy": "2025-07-16T09:41:34.416829Z",
          "iopub.status.idle": "2025-07-16T09:41:57.126731Z",
          "shell.execute_reply": "2025-07-16T09:41:57.125831Z",
          "shell.execute_reply.started": "2025-07-16T09:41:34.417063Z"
        },
        "id": "l4QClN06zCLD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "import flwr\n",
        "from flwr.client import Client, ClientApp, NumPyClient\n",
        "from flwr.common import Context\n",
        "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
        "from flwr.server.strategy import Strategy\n",
        "from flwr.simulation import run_simulation\n",
        "from flwr_datasets import FederatedDataset\n",
        "\n",
        "from flwr_datasets import FederatedDataset\n",
        "from flwr_datasets.visualization import plot_label_distributions\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from flwr_datasets import FederatedDataset\n",
        "from flwr_datasets.partitioner import DirichletPartitioner\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on {DEVICE}\")\n",
        "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZsuXEDOPSXK"
      },
      "source": [
        "## Base config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:41:57.128587Z",
          "iopub.status.busy": "2025-07-16T09:41:57.127943Z",
          "iopub.status.idle": "2025-07-16T09:41:57.132882Z",
          "shell.execute_reply": "2025-07-16T09:41:57.132028Z",
          "shell.execute_reply.started": "2025-07-16T09:41:57.128566Z"
        },
        "id": "cqhXjWssParm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CACHE_DIR = \"/kaggle/working/hf_cache\"\n",
        "os.environ[\"HF_DATASETS_OFFLINE\"] = \"0\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:41:57.133947Z",
          "iopub.status.busy": "2025-07-16T09:41:57.133708Z",
          "iopub.status.idle": "2025-07-16T09:41:57.183039Z",
          "shell.execute_reply": "2025-07-16T09:41:57.182129Z",
          "shell.execute_reply.started": "2025-07-16T09:41:57.133929Z"
        },
        "id": "gf2sW09C7bt2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "backend_config = {\"client_resources\": {\"num_cpus\": 1}}\n",
        "if DEVICE.type == \"cuda\":\n",
        "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1}}  # GPU optional\n",
        "\n",
        "NUM_PARTITIONS = 20\n",
        "NUM_ROUND = 1\n",
        "EPOCH = 2\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "\n",
        "FRACTION_FIT=0.85\n",
        "FRACTION_EVALUATE=0.3\n",
        "MIN_FIT_CLIENTS=7\n",
        "MIN_EVALUATE_CLIENTS=10\n",
        "MIN_AVAILABLE_CLIENTS=10\n",
        "\n",
        "PROX_MU = 0.01 # Tham số Mu tốt nhất\n",
        "\n",
        "ALPHA = 0.1\n",
        "\n",
        "CLIENT_LR = 10**(-2.5)\n",
        "SERVER_LR = 10**(-2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkZNBQXyEOGL"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:41:57.185453Z",
          "iopub.status.busy": "2025-07-16T09:41:57.185170Z",
          "iopub.status.idle": "2025-07-16T09:41:57.202534Z",
          "shell.execute_reply": "2025-07-16T09:41:57.201882Z",
          "shell.execute_reply.started": "2025-07-16T09:41:57.185433Z"
        },
        "id": "lY9VmzmGhiyJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:41:57.203923Z",
          "iopub.status.busy": "2025-07-16T09:41:57.203598Z",
          "iopub.status.idle": "2025-07-16T09:41:57.216812Z",
          "shell.execute_reply": "2025-07-16T09:41:57.215942Z",
          "shell.execute_reply.started": "2025-07-16T09:41:57.203894Z"
        },
        "id": "wWU1X8UyJETA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from flwr_datasets import FederatedDataset\n",
        "from flwr_datasets.partitioner import DirichletPartitioner\n",
        "\n",
        "# Cập nhật transforms cho CIFAR-10 với ResNet50\n",
        "def load_datasets(partition_id, num_partitions: int):\n",
        "    # Tạo dữ liệu Non-IID\n",
        "    partitioner = DirichletPartitioner(\n",
        "        num_partitions=NUM_PARTITIONS,\n",
        "        partition_by=\"label\",\n",
        "        alpha=ALPHA,\n",
        "        min_partition_size=num_partitions,\n",
        "        self_balancing=True\n",
        "    )\n",
        "\n",
        "    fds = FederatedDataset(\n",
        "        dataset=\"cifar10\",\n",
        "        partitioners={\"train\": partitioner},\n",
        "        cache_dir=CACHE_DIR\n",
        "    )\n",
        "\n",
        "    partition = fds.load_partition(partition_id)\n",
        "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "    # Transforms được tối ưu cho ResNet18 + FL\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize((112, 112)),  # Nhỏ hơn, tiết kiệm computation\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=5),  # Giảm từ 10 xuống 5\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1),  # Thêm augmentation nhẹ\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transforms = transforms.Compose([\n",
        "        transforms.Resize((112, 112)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    def apply_train_transforms(batch):\n",
        "        try:\n",
        "            batch[\"img\"] = [train_transforms(img.convert(\"RGB\")) for img in batch[\"img\"]]\n",
        "            return batch\n",
        "        except Exception as e:\n",
        "            print(f\"Error in train transforms: {e}\")\n",
        "            raise\n",
        "\n",
        "    def apply_val_transforms(batch):\n",
        "        try:\n",
        "            batch[\"img\"] = [val_transforms(img.convert(\"RGB\")) for img in batch[\"img\"]]\n",
        "            return batch\n",
        "        except Exception as e:\n",
        "            print(f\"Error in val transforms: {e}\")\n",
        "            raise\n",
        "\n",
        "    # Apply transforms\n",
        "    partition_train_test[\"train\"] = partition_train_test[\"train\"].with_transform(apply_train_transforms)\n",
        "    partition_train_test[\"test\"] = partition_train_test[\"test\"].with_transform(apply_val_transforms)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pin_memory = torch.cuda.is_available()\n",
        "    num_workers = 0  # Luôn để 0 để tránh bug multiprocessing\n",
        "\n",
        "    trainloader = DataLoader(\n",
        "        partition_train_test[\"train\"],\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers,\n",
        "        persistent_workers=False,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    valloader = DataLoader(\n",
        "        partition_train_test[\"test\"],\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers,\n",
        "        persistent_workers=False,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "\n",
        "    testset = fds.load_split(\"test\").with_transform(apply_val_transforms)\n",
        "    testloader = DataLoader(\n",
        "        testset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers,\n",
        "        persistent_workers=False,  # num_workers=0 thì phải là False\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    return trainloader, valloader, testloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:41:57.217925Z",
          "iopub.status.busy": "2025-07-16T09:41:57.217637Z",
          "iopub.status.idle": "2025-07-16T09:42:41.604606Z",
          "shell.execute_reply": "2025-07-16T09:42:41.604005Z",
          "shell.execute_reply.started": "2025-07-16T09:41:57.217905Z"
        },
        "id": "cAbwmhWqPhr-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"\")\n",
        "all_data = [load_datasets(i, NUM_PARTITIONS) for i in range(NUM_PARTITIONS)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:42:41.605787Z",
          "iopub.status.busy": "2025-07-16T09:42:41.605546Z",
          "iopub.status.idle": "2025-07-16T09:42:44.503436Z",
          "shell.execute_reply": "2025-07-16T09:42:44.502569Z",
          "shell.execute_reply.started": "2025-07-16T09:42:41.605766Z"
        },
        "id": "8dfh64QIjtLz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from flwr_datasets import FederatedDataset\n",
        "from flwr_datasets.visualization import plot_label_distributions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tạo partitioner\n",
        "partitioner = DirichletPartitioner(\n",
        "    num_partitions=NUM_PARTITIONS,\n",
        "    partition_by=\"label\",\n",
        "    alpha=ALPHA,\n",
        "    min_partition_size=NUM_PARTITIONS,\n",
        "    self_balancing=True\n",
        ")\n",
        "\n",
        "# Tạo FederatedDataset\n",
        "fds = FederatedDataset(\n",
        "    dataset=\"cifar10\",\n",
        "    partitioners={\"train\": partitioner},\n",
        "    cache_dir=CACHE_DIR\n",
        ")\n",
        "\n",
        "# Visualize phân bố\n",
        "print(f\"Visualizing Non-IID distribution with alpha={ALPHA}\")\n",
        "partitioner = fds.partitioners[\"train\"]\n",
        "figure, axis, dataframe = plot_label_distributions(\n",
        "    partitioner=partitioner,\n",
        "    label_name=\"label\",\n",
        "    legend=True,\n",
        "    verbose_labels=True,\n",
        "    plot_type=\"bar\",  # Thêm này để rõ hơn\n",
        "    size_unit=\"absolute\"  # Hoặc \"percentage\"\n",
        ")\n",
        "\n",
        "# Customize plot\n",
        "plt.title(f'CIFAR-10 Non-IID Distribution (α={ALPHA})')\n",
        "plt.xlabel('Clients')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# In thống kê\n",
        "print(\"\\n=== Distribution Statistics ===\")\n",
        "print(f\"Total clients: {NUM_PARTITIONS}\")\n",
        "print(f\"Alpha (non-IID level): {ALPHA}\")\n",
        "print(f\"Dataset: CIFAR-10\")\n",
        "print(f\"Classes: {dataframe.columns.tolist()}\")\n",
        "\n",
        "# Thống kê chi tiết\n",
        "total_samples = dataframe.sum().sum()\n",
        "print(f\"Total samples: {total_samples}\")\n",
        "print(f\" samples per client: {total_samples/NUM_PARTITIONS:.1f}\")\n",
        "print(f\"Min samples: {dataframe.sum(axis=1).min()}\")\n",
        "print(f\"Max samples: {dataframe.sum(axis=1).max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:42:44.505156Z",
          "iopub.status.busy": "2025-07-16T09:42:44.504609Z",
          "iopub.status.idle": "2025-07-16T09:42:44.522834Z",
          "shell.execute_reply": "2025-07-16T09:42:44.521704Z",
          "shell.execute_reply.started": "2025-07-16T09:42:44.505137Z"
        },
        "id": "LKkSYTKijchR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Function to calculate entropy for a list of counts\n",
        "def calculate_entropy(counts):\n",
        "    total = sum(counts)\n",
        "    if total == 0:\n",
        "        return 0\n",
        "    probabilities = [c / total for c in counts]\n",
        "    return entropy(probabilities, base=2) # Using base 2 for bits\n",
        "\n",
        "entropies = []\n",
        "for client_id in range(NUM_PARTITIONS):\n",
        "    label_counts = dataframe.iloc[client_id].tolist() # Get label counts for this client\n",
        "    entropies.append(calculate_entropy(label_counts))\n",
        "\n",
        "average_entropy = np.mean(entropies)\n",
        "std_entropy = np.std(entropies)\n",
        "\n",
        "print(f\"Average Entropy: {average_entropy:.4f}\")\n",
        "print(f\"Entropy Std: {std_entropy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKbsqaR1EWaf"
      },
      "source": [
        "## Model ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:42:44.524389Z",
          "iopub.status.busy": "2025-07-16T09:42:44.524117Z",
          "iopub.status.idle": "2025-07-16T09:42:44.550449Z",
          "shell.execute_reply": "2025-07-16T09:42:44.549701Z",
          "shell.execute_reply.started": "2025-07-16T09:42:44.524368Z"
        },
        "id": "ZsdWl8gLyKVb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class ResNet18_GroupNorm(nn.Module):\n",
        "    def __init__(self, num_classes: int = 10, pretrained: bool = True):\n",
        "        super(ResNet18_GroupNorm, self).__init__()\n",
        "\n",
        "        # Sử dụng pretrained weights\n",
        "        weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        self.backbone = models.resnet18(weights=weights)\n",
        "\n",
        "        # Thay BatchNorm bằng GroupNorm (2 nhóm/lớp)\n",
        "        self._replace_batchnorm_with_groupnorm(self.backbone)\n",
        "\n",
        "        # Lấy số feature từ fc gốc\n",
        "        feat_dim = self.backbone.fc.in_features\n",
        "\n",
        "        # Bỏ lớp fc gốc\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        # Classifier với GroupNorm\n",
        "        self.classifier = nn.Sequential(\n",
        "            # Thêm dropout để tránh overfitting\n",
        "            nn.Dropout(0.3),  # Dropout sau feature extraction\n",
        "            nn.Linear(feat_dim, 128),\n",
        "            nn.GroupNorm(2, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),  # Dropout giữa các lớp\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.GroupNorm(2, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.1),  # Dropout nhẹ hơn ở cuối\n",
        "\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _replace_batchnorm_with_groupnorm(self, module):\n",
        "        \"\"\"Thay tất cả BatchNorm bằng GroupNorm với 2 nhóm\"\"\"\n",
        "        for name, child in module.named_children():\n",
        "            if isinstance(child, nn.BatchNorm2d):\n",
        "                num_channels = child.num_features\n",
        "                # Thay BatchNorm2d bằng GroupNorm với 2 nhóm\n",
        "                setattr(module, name, nn.GroupNorm(2, num_channels))\n",
        "            elif isinstance(child, nn.BatchNorm1d):\n",
        "                num_channels = child.num_features\n",
        "                # Thay BatchNorm1d bằng GroupNorm với 2 nhóm\n",
        "                setattr(module, name, nn.GroupNorm(2, num_channels))\n",
        "            else:\n",
        "                # Đệ quy cho các module con\n",
        "                self._replace_batchnorm_with_groupnorm(child)\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.classifier:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "def train_fedprox(net, trainloader, epochs: int, global_params=None, mu=PROX_MU, lr=0.01, weight_decay=1e-3):\n",
        "    \"\"\"\n",
        "    Train local model with FedProx proximal term.\n",
        "    Args:\n",
        "        net: local model (nn.Module)\n",
        "        trainloader: DataLoader for local data\n",
        "        epochs: number of epochs to train\n",
        "        global_params: list of global model parameters (from server, after FedAvg)\n",
        "        mu: proximal term coefficient (float)\n",
        "        lr: learning rate (float)\n",
        "        weight_decay: L2 regularization for optimizer\n",
        "    Returns:\n",
        "        epoch_loss: average loss after last epoch\n",
        "        epoch_acc: average accuracy after last epoch\n",
        "    \"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    net.train()\n",
        "\n",
        "    device = next(net.parameters()).device\n",
        "\n",
        "    # Lưu lại global params dưới dạng detached clone để tính toán (tránh bị update khi backward)\n",
        "    if global_params is not None:\n",
        "        global_params = [p.detach().clone() for p in global_params]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for batch in trainloader:\n",
        "            images, labels = batch[\"img\"].to(device), batch[\"label\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # THÊM PROXIMAL TERM\n",
        "            if global_params is not None and mu > 0:\n",
        "                prox_loss = 0.0\n",
        "                for param, global_param in zip(net.parameters(), global_params):\n",
        "                    prox_loss += torch.norm(param - global_param) ** 2\n",
        "                loss += (mu / 2) * prox_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * labels.size(0)\n",
        "            total += labels.size(0)\n",
        "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "        epoch_loss /= total\n",
        "        epoch_acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}: train loss {epoch_loss:.4f}, accuracy {epoch_acc:.4f}\")\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def test(net, testloader):\n",
        "    \"\"\"Đánh giá mô hình trên tập validation và trả về loss, acc.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, total_loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in testloader:\n",
        "            images, labels = batch[\"img\"], batch[\"label\"]\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    prox_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    return prox_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:42:44.551554Z",
          "iopub.status.busy": "2025-07-16T09:42:44.551261Z",
          "iopub.status.idle": "2025-07-16T09:42:44.569218Z",
          "shell.execute_reply": "2025-07-16T09:42:44.568432Z",
          "shell.execute_reply.started": "2025-07-16T09:42:44.551525Z"
        },
        "id": "-khbcU7PZidi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    params = []\n",
        "    for name, val in net.state_dict().items():\n",
        "        if val.numel() > 0:\n",
        "            params.append(val.cpu().numpy())\n",
        "        else:\n",
        "            print(f\"Warning: Empty tensor found: {name}\")\n",
        "    return params\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    if not parameters:\n",
        "        print(\"Warning: No parameters received\")\n",
        "        return\n",
        "\n",
        "    state_dict_keys = list(net.state_dict().keys())\n",
        "    print(f\"Model has {len(state_dict_keys)} parameters\")\n",
        "    print(f\"Received {len(parameters)} parameters\")\n",
        "\n",
        "    if len(parameters) != len(state_dict_keys):\n",
        "        print(\"Parameter count mismatch!\")\n",
        "        return\n",
        "\n",
        "    params_dict = zip(state_dict_keys, parameters)\n",
        "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
        "    net.load_state_dict(state_dict, strict=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wZKWVLJRFfd"
      },
      "source": [
        "## Custom FedProx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:42:44.570289Z",
          "iopub.status.busy": "2025-07-16T09:42:44.570040Z",
          "iopub.status.idle": "2025-07-16T09:42:44.589378Z",
          "shell.execute_reply": "2025-07-16T09:42:44.588711Z",
          "shell.execute_reply.started": "2025-07-16T09:42:44.570268Z"
        },
        "id": "GpUj5AYqOUX0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class FlowerClientFedProx(NumPyClient):\n",
        "    def __init__(self, partition_id, net, trainloader, valloader):\n",
        "        self.partition_id = partition_id\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return get_parameters(self.net)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        print(f\"[Client {self.partition_id}] FedProx fit\")\n",
        "\n",
        "        # Lưu global parameters để tính proximal term\n",
        "        global_params = [torch.tensor(param).to(DEVICE) for param in parameters]\n",
        "\n",
        "        set_parameters(self.net, parameters)\n",
        "\n",
        "        # Lấy proximal_mu từ config hoặc dùng default\n",
        "        mu = config.get(\"proximal_mu\", 0.1)\n",
        "\n",
        "        # Sử dụng training function có proximal regularization\n",
        "        loss, acc = train_fedprox(self.net, self.trainloader, epochs=EPOCH,\n",
        "                                 global_params=global_params, mu=mu)\n",
        "\n",
        "        return get_parameters(self.net), len(self.trainloader.dataset), {\n",
        "            \"loss\": float(loss), \"accuracy\": float(acc)\n",
        "        }\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        set_parameters(self.net, parameters)\n",
        "        loss, accuracy = test(self.net, self.valloader)\n",
        "        return float(loss), len(self.valloader.dataset), {\n",
        "            \"loss\": float(loss), \"accuracy\": float(accuracy)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:42:44.590432Z",
          "iopub.status.busy": "2025-07-16T09:42:44.590199Z",
          "iopub.status.idle": "2025-07-16T09:42:44.607675Z",
          "shell.execute_reply": "2025-07-16T09:42:44.606830Z",
          "shell.execute_reply.started": "2025-07-16T09:42:44.590412Z"
        },
        "id": "DREntS1NOcfW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def client_fn_fedprox(context):\n",
        "    cid = context.node_config[\"partition-id\"]\n",
        "    trainloader, valloader, _ = all_data[cid]\n",
        "\n",
        "    net = ResNet18_GroupNorm(num_classes=10).to(DEVICE)\n",
        "    return FlowerClientFedProx(cid, net, trainloader, valloader).to_client()\n",
        "\n",
        "client_app_fedprox = ClientApp(client_fn=client_fn_fedprox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:42:44.610704Z",
          "iopub.status.busy": "2025-07-16T09:42:44.610453Z",
          "iopub.status.idle": "2025-07-16T09:42:45.457801Z",
          "shell.execute_reply": "2025-07-16T09:42:45.457099Z",
          "shell.execute_reply.started": "2025-07-16T09:42:44.610679Z"
        },
        "id": "N-Lhg6UaXCpy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from flwr.common import Metrics, Context\n",
        "from flwr.common import ndarrays_to_parameters\n",
        "\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    # Multiply accuracy of each client by number of examples used\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    # Aggregate and return custom metric (weighted average)\n",
        "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = ResNet18_GroupNorm().to(DEVICE)\n",
        "initial_parameters = flwr.common.ndarrays_to_parameters(get_parameters(net))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-16T09:42:45.458969Z",
          "iopub.status.busy": "2025-07-16T09:42:45.458687Z",
          "iopub.status.idle": "2025-07-16T09:42:45.469473Z",
          "shell.execute_reply": "2025-07-16T09:42:45.468621Z",
          "shell.execute_reply.started": "2025-07-16T09:42:45.458946Z"
        },
        "id": "2h-sLVvnRG-Z",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from flwr.server.strategy import FedProx\n",
        "import time\n",
        "\n",
        "from flwr.server.strategy import FedProx\n",
        "import time\n",
        "\n",
        "class CustomFedProx(FedProx):\n",
        "    def __init__(self, proximal_mu=0.1, *args, **kwargs):\n",
        "        super().__init__(proximal_mu=proximal_mu, *args, **kwargs)\n",
        "        self.metrics_centralized = []\n",
        "        self.round_start_times = {}  # Để tính thời gian của từng round\n",
        "\n",
        "    def configure_fit(self, server_round, parameters, client_manager):\n",
        "        # Ghi nhận thời gian bắt đầu round\n",
        "        self.round_start_times[server_round] = time.time()\n",
        "\n",
        "        config = {\"proximal_mu\": self.proximal_mu}\n",
        "        fit_ins = flwr.common.FitIns(parameters, config)\n",
        "\n",
        "        sample_size, min_num_clients = self.num_fit_clients(client_manager.num_available())\n",
        "        clients = client_manager.sample(num_clients=sample_size, min_num_clients=min_num_clients)\n",
        "\n",
        "        return [(client, fit_ins) for client in clients]\n",
        "\n",
        "    def aggregate_fit(self, rnd, results, failures):\n",
        "        aggregated_parameters, _ = super().aggregate_fit(rnd, results, failures)\n",
        "\n",
        "        losses = [r.metrics.get(\"loss\") for _, r in results if r.metrics and \"loss\" in r.metrics]\n",
        "        accs = [r.metrics.get(\"accuracy\") for _, r in results if r.metrics and \"accuracy\" in r.metrics]\n",
        "\n",
        "        prox_loss = sum(losses) / len(losses) if losses else None\n",
        "        prox_acc = sum(accs) / len(accs) if accs else None\n",
        "\n",
        "        print(f\"[Round {rnd}] Train — Loss: {prox_loss:.4f}, Acc: {prox_acc:.4f}\" if prox_loss is not None else\n",
        "              f\"[Round {rnd}] Train — Loss: N/A, Acc: N/A\")\n",
        "\n",
        "        while len(self.metrics_centralized) < rnd:\n",
        "            self.metrics_centralized.append({})\n",
        "\n",
        "        self.metrics_centralized[rnd - 1].update({\n",
        "            \"train_loss\": prox_loss,\n",
        "            \"train_acc\": prox_acc\n",
        "        })\n",
        "\n",
        "        return aggregated_parameters, {}\n",
        "\n",
        "    def aggregate_evaluate(self, rnd, results, failures):\n",
        "        eval_start = time.time()\n",
        "        aggregated_loss, _ = super().aggregate_evaluate(rnd, results, failures)\n",
        "        eval_duration = time.time() - eval_start\n",
        "\n",
        "        round_total_time = time.time() - self.round_start_times.get(rnd, time.time())\n",
        "\n",
        "        losses = [r.metrics.get(\"loss\") for _, r in results if r.metrics and \"loss\" in r.metrics]\n",
        "        accs = [r.metrics.get(\"accuracy\") for _, r in results if r.metrics and \"accuracy\" in r.metrics]\n",
        "\n",
        "        prox_loss = sum(losses) / len(losses) if losses else None\n",
        "        prox_acc = sum(accs) / len(accs) if accs else None\n",
        "\n",
        "        print(f\"[Round {rnd}] Val   — Loss: {prox_loss:.4f}, Acc: {prox_acc:.4f}, Time: {eval_duration:.2f}s\")\n",
        "        print(f\"[Round {rnd}] Total Round Time: {round_total_time:.2f}s\")\n",
        "\n",
        "        while len(self.metrics_centralized) < rnd:\n",
        "            self.metrics_centralized.append({})\n",
        "\n",
        "        self.metrics_centralized[rnd - 1].update({\n",
        "            \"val_loss\": prox_loss,\n",
        "            \"val_acc\": prox_acc,\n",
        "            \"eval_duration\": eval_duration,\n",
        "            \"round_total_time\": round_total_time,\n",
        "            \"round_duration\": round_total_time  # fallback để JSON unified hỗ trợ đầy đủ\n",
        "        })\n",
        "\n",
        "        return aggregated_loss, {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RezzLAcM4xC",
        "outputId": "e65a27b6-21bd-4a0a-e8a1-2fe0f7dec8a7"
      },
      "outputs": [],
      "source": [
        "# Tạo strategy cho FedProx\n",
        "strategyProx = CustomFedProx(\n",
        "    fraction_fit=FRACTION_FIT,\n",
        "    fraction_evaluate=FRACTION_EVALUATE,\n",
        "    min_fit_clients=MIN_FIT_CLIENTS,\n",
        "    min_evaluate_clients=MIN_EVALUATE_CLIENTS,\n",
        "    min_available_clients=MIN_AVAILABLE_CLIENTS,\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,\n",
        "    initial_parameters=initial_parameters,\n",
        "    proximal_mu=PROX_MU,\n",
        ")\n",
        "\n",
        "def server_fn(context: Context) -> ServerAppComponents:\n",
        "    config = ServerConfig(num_rounds=NUM_ROUND)\n",
        "    return ServerAppComponents(strategy=strategyProx, config=config)\n",
        "\n",
        "# Tạo server app mới với FedProx\n",
        "server = ServerApp(server_fn=server_fn)\n",
        "\n",
        "# Run simulation với client FedProx\n",
        "run_simulation(\n",
        "    server_app=server,\n",
        "    client_app=client_app_fedprox,    # đổi lại đúng client_app của bạn cho FedProx\n",
        "    num_supernodes=NUM_PARTITIONS,\n",
        "    backend_config=backend_config,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "6hUc5uTiVMb1",
        "outputId": "6d47b565-daa9-48c6-adaa-512b4f8771b4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# =============================================================================\n",
        "# FUNCTIONS FOR SAVING RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def save_results_to_json_unified(strategy, strategy_name, save_dir=\"fl_results\"):\n",
        "    \"\"\"\n",
        "    Lưu kết quả huấn luyện của strategy với định dạng JSON chuẩn hóa (không bao gồm fit_duration).\n",
        "    Áp dụng được cho FedAvg, FedAdam, FedProx và các strategy khác.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    unified_metrics = []\n",
        "    for i, metrics in enumerate(strategy.metrics_centralized):\n",
        "        unified_metrics.append({\n",
        "            \"round\": i + 1,\n",
        "            \"train_loss\": metrics.get(\"train_loss\"),\n",
        "            \"train_acc\": metrics.get(\"train_acc\"),\n",
        "            \"val_loss\": metrics.get(\"val_loss\"),\n",
        "            \"val_acc\": metrics.get(\"val_acc\"),\n",
        "            \"eval_duration\": metrics.get(\"eval_duration\"),  # Có thể là None\n",
        "            \"round_total_time\": metrics.get(\"round_total_time\", metrics.get(\"round_duration\"))  # fallback nếu cần\n",
        "        })\n",
        "\n",
        "    # Runtime tổng hợp\n",
        "    round_times = [m[\"round_total_time\"] for m in unified_metrics if m[\"round_total_time\"] is not None]\n",
        "    total_runtime = sum(round_times)\n",
        "    prox_runtime = total_runtime / len(round_times) if round_times else 0\n",
        "\n",
        "    results = {\n",
        "        \"strategy_name\": strategy_name,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"metrics\": unified_metrics,\n",
        "        \"runtime_summary\": {\n",
        "            \"total_runtime_seconds\": total_runtime,\n",
        "            \"average_round_time_seconds\": prox_runtime,\n",
        "            \"fastest_round_seconds\": min(round_times) if round_times else None,\n",
        "            \"slowest_round_seconds\": max(round_times) if round_times else None,\n",
        "            \"num_rounds\": len(unified_metrics)\n",
        "        },\n",
        "        \"config\": {\n",
        "            \"num_rounds\": len(unified_metrics),\n",
        "            \"num_partitions\": globals().get(\"NUM_PARTITIONS\", None),\n",
        "            \"batch_size\": globals().get(\"BATCH_SIZE\", None),\n",
        "            \"learning_rate\": globals().get(\"CLIENT_LR\", None),\n",
        "            \"epochs_per_round\": globals().get(\"EPOCH\", None)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    filename = f\"{save_dir}/{strategy_name}_{timestamp}.json\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Saved unified JSON (compatible with all strategies) for {strategy_name} to {filename}\")\n",
        "    return filename\n",
        "\n",
        "\n",
        "def save_results_to_pickle(strategy, strategy_name, save_dir=\"fl_results\"):\n",
        "    \"\"\"\n",
        "    Lưu kết quả của strategy vào file pickle (có thể lưu được object phức tạp)\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    results = {\n",
        "        \"strategy_name\": strategy_name,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"strategy_object\": strategy,  # Lưu cả object\n",
        "        \"metrics\": strategy.metrics_centralized,\n",
        "        \"config\": {\n",
        "            \"num_rounds\": len(strategy.metrics_centralized),\n",
        "            \"num_partitions\": NUM_PARTITIONS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"learning_rate\": CLIENT_LR,\n",
        "            \"epochs_per_round\": EPOCH\n",
        "        }\n",
        "    }\n",
        "\n",
        "    filename = f\"{save_dir}/{strategy_name}_{timestamp}.pkl\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(results, f)\n",
        "\n",
        "    print(f\"✅ Saved {strategy_name} results to {filename}\")\n",
        "    return filename\n",
        "\n",
        "def save_results_to_csv(strategy, strategy_name, save_dir=\"fl_results\"):\n",
        "    \"\"\"\n",
        "    Lưu kết quả của strategy vào file CSV. Hoạt động tốt cho mọi strategy, kể cả FedProx.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    metrics_data = []\n",
        "    for round_idx, metrics in enumerate(strategy.metrics_centralized):\n",
        "        row = {\n",
        "            \"round\": round_idx + 1,\n",
        "            \"train_loss\": metrics.get(\"train_loss\"),\n",
        "            \"train_acc\": metrics.get(\"train_acc\"),\n",
        "            \"val_loss\": metrics.get(\"val_loss\"),\n",
        "            \"val_acc\": metrics.get(\"val_acc\"),\n",
        "            \"round_total_time\": metrics.get(\"round_total_time\", metrics.get(\"round_duration\", None)),\n",
        "            \"strategy\": strategy_name,\n",
        "            \"timestamp\": timestamp\n",
        "        }\n",
        "        metrics_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(metrics_data)\n",
        "\n",
        "    filename = f\"{save_dir}/{strategy_name}_{timestamp}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"✅ Saved {strategy_name} results to {filename}\")\n",
        "    return filename\n",
        "\n",
        "\n",
        "def save_all_strategies_comparison(strategies_dict, save_dir=\"fl_results\"):\n",
        "    \"\"\"\n",
        "    Lưu so sánh tất cả strategies vào một file CSV\n",
        "\n",
        "    Args:\n",
        "        strategies_dict: Dict chứa {strategy_name: strategy_object}\n",
        "        save_dir: Thư mục lưu kết quả\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for strategy_name, strategy in strategies_dict.items():\n",
        "        for round_idx, metrics in enumerate(strategy.metrics_centralized):\n",
        "            row = {\n",
        "                \"round\": round_idx + 1,\n",
        "                \"train_loss\": metrics.get(\"train_loss\"),\n",
        "                \"train_acc\": metrics.get(\"train_acc\"),\n",
        "                \"val_loss\": metrics.get(\"val_loss\"),\n",
        "                \"val_acc\": metrics.get(\"val_acc\"),\n",
        "                \"strategy\": strategy_name,\n",
        "                \"timestamp\": timestamp\n",
        "            }\n",
        "            all_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    filename = f\"{save_dir}/comparison_{timestamp}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"✅ Saved comparison results to {filename}\")\n",
        "    return filename\n",
        "\n",
        "# =============================================================================\n",
        "# FUNCTIONS FOR LOADING RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "def load_results_from_json(filename):\n",
        "    \"\"\"Load kết quả từ file JSON\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        results = json.load(f)\n",
        "    print(f\"✅ Loaded results from {filename}\")\n",
        "    return results\n",
        "\n",
        "def load_results_from_pickle(filename):\n",
        "    \"\"\"Load kết quả từ file pickle\"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        results = pickle.load(f)\n",
        "    print(f\"✅ Loaded results from {filename}\")\n",
        "    return results\n",
        "\n",
        "def load_results_from_csv(filename):\n",
        "    \"\"\"Load kết quả từ file CSV\"\"\"\n",
        "    df = pd.read_csv(filename)\n",
        "    print(f\"✅ Loaded results from {filename}\")\n",
        "    return df\n",
        "\n",
        "# =============================================================================\n",
        "# VISUALIZATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def plot_comparison_from_csv(csv_filename, save_plot=True):\n",
        "    \"\"\"\n",
        "    Tạo biểu đồ so sánh từ file CSV\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Tạo subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Train Loss\n",
        "    for strategy in df['strategy'].unique():\n",
        "        strategy_data = df[df['strategy'] == strategy]\n",
        "        axes[0, 0].plot(strategy_data['round'], strategy_data['train_loss'],\n",
        "                       label=f'{strategy}', marker='o')\n",
        "    axes[0, 0].set_title('Train Loss Comparison')\n",
        "    axes[0, 0].set_xlabel('Round')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    # Val Loss\n",
        "    for strategy in df['strategy'].unique():\n",
        "        strategy_data = df[df['strategy'] == strategy]\n",
        "        axes[0, 1].plot(strategy_data['round'], strategy_data['val_loss'],\n",
        "                       label=f'{strategy}', marker='s')\n",
        "    axes[0, 1].set_title('Validation Loss Comparison')\n",
        "    axes[0, 1].set_xlabel('Round')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    # Train Accuracy\n",
        "    for strategy in df['strategy'].unique():\n",
        "        strategy_data = df[df['strategy'] == strategy]\n",
        "        axes[1, 0].plot(strategy_data['round'], strategy_data['train_acc'],\n",
        "                       label=f'{strategy}', marker='o')\n",
        "    axes[1, 0].set_title('Train Accuracy Comparison')\n",
        "    axes[1, 0].set_xlabel('Round')\n",
        "    axes[1, 0].set_ylabel('Accuracy')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "\n",
        "    # Val Accuracy\n",
        "    for strategy in df['strategy'].unique():\n",
        "        strategy_data = df[df['strategy'] == strategy]\n",
        "        axes[1, 1].plot(strategy_data['round'], strategy_data['val_acc'],\n",
        "                       label=f'{strategy}', marker='s')\n",
        "    axes[1, 1].set_title('Validation Accuracy Comparison')\n",
        "    axes[1, 1].set_xlabel('Round')\n",
        "    axes[1, 1].set_ylabel('Accuracy')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_plot:\n",
        "        plot_filename = csv_filename.replace('.csv', '_comparison_plo_100.png')\n",
        "        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "        print(f\"✅ Saved plot to {plot_filename}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def create_summary_report(strategies_dict, save_dir=\"fl_results\"):\n",
        "    \"\"\"\n",
        "    Tạo báo cáo tóm tắt cho tất cả strategies. Tự động fallback nếu thiếu runtime.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    summary_data = []\n",
        "\n",
        "    for strategy_name, strategy in strategies_dict.items():\n",
        "        metrics = strategy.metrics_centralized\n",
        "\n",
        "        final_round = len(metrics)\n",
        "        final_train_acc = metrics[-1].get(\"train_acc\", 0) if metrics else 0\n",
        "        final_val_acc = metrics[-1].get(\"val_acc\", 0) if metrics else 0\n",
        "        final_train_loss = metrics[-1].get(\"train_loss\", 0) if metrics else 0\n",
        "        final_val_loss = metrics[-1].get(\"val_loss\", 0) if metrics else 0\n",
        "\n",
        "        val_accs = [m.get(\"val_acc\", 0) for m in metrics if m.get(\"val_acc\") is not None]\n",
        "        best_val_acc = max(val_accs) if val_accs else 0\n",
        "        best_val_round = val_accs.index(best_val_acc) + 1 if val_accs else 0\n",
        "\n",
        "        round_times = [\n",
        "            m.get(\"round_total_time\", m.get(\"round_duration\"))\n",
        "            for m in metrics if m.get(\"round_total_time\") or m.get(\"round_duration\")\n",
        "        ]\n",
        "        total_runtime = sum(round_times)\n",
        "        prox_round_time = total_runtime / len(round_times) if round_times else 0\n",
        "        max_round_time = max(round_times) if round_times else 0\n",
        "        min_round_time = min(round_times) if round_times else 0\n",
        "\n",
        "\n",
        "        summary_data.append({\n",
        "            \"strategy\": strategy_name,\n",
        "            \"final_round\": final_round,\n",
        "            \"final_train_acc\": final_train_acc,\n",
        "            \"final_val_acc\": final_val_acc,\n",
        "            \"final_train_loss\": final_train_loss,\n",
        "            \"final_val_loss\": final_val_loss,\n",
        "            \"best_val_acc\": best_val_acc,\n",
        "            \"best_val_round\": best_val_round,\n",
        "            \"total_runtime_sec\": total_runtime,\n",
        "            \"total_runtime_min\": total_runtime / 60,\n",
        "            \"prox_round_time_sec\": prox_round_time,\n",
        "            \"max_round_time_sec\": max_round_time,\n",
        "            \"min_round_time_sec\": min_round_time,\n",
        "            \"timestamp\": timestamp\n",
        "        })\n",
        "\n",
        "    df_summary = pd.DataFrame(summary_data)\n",
        "\n",
        "    filename = f\"{save_dir}/summary_report_{timestamp}.csv\"\n",
        "    df_summary.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"✅ Saved summary report to {filename}\")\n",
        "    print(\"\\n📊 SUMMARY REPORT:\")\n",
        "    print(df_summary.to_string(index=False))\n",
        "\n",
        "    print(\"\\n⏱️ RUNTIME SUMMARY:\")\n",
        "    for _, row in df_summary.iterrows():\n",
        "        print(f\"{row['strategy']}:\")\n",
        "        print(f\"  Total time: {row['total_runtime_min']:.1f}min ({row['total_runtime_sec']:.1f}s)\")\n",
        "        print(f\"  Prox round: {row['prox_round_time_sec']:.2f}s\")\n",
        "        print(f\"  Range: {row['min_round_time_sec']:.2f}s - {row['max_round_time_sec']:.2f}s\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "save_results_to_json_unified(strategyProx, \"FedProx\")\n",
        "save_results_to_csv(strategyProx, \"FedProx\")\n",
        "\n",
        "strategies_dict = {\n",
        "    \"FedProx\": strategyProx,\n",
        "}\n",
        "\n",
        "create_summary_report(strategies_dict)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
